![GitHub License](https://img.shields.io/github/license/UK-07/Tokenizer)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RQCWVFGnsM7FaWe93Zvgp0WP6Il3w-sY)

# Sub-work Tokenizer
This project implements a sub-word tokenizer typically used for training transformer models. It includes a notebook to illustrate how Python deals with Unicode and the complete implementation of a Byte-Pair Encoding based sub-word tokenizer.

## Requirements
* Python 3.9+
* PyTorch

## References
* [OpenAI TikToken](https://github.com/openai/tiktoken)
* [Google SentencePiece](https://github.com/google/sentencepiece)
* [Andrej Karpathy's BPE](https://github.com/karpathy/minbpe)
